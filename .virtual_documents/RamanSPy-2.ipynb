








!pip install ramanspy
!pip install adjustText

!git clone https://github.com/ARIM-Academy/Advanced_Tutorial_5.git
%cd Advanced_Tutorial_5





from pathlib import Path

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# ラマンの前処理
import ramanspy as rp

# 警告を無視
from warnings import filterwarnings
filterwarnings('ignore')








def read_csv(csv_filename, skiprows):
    
    """
    概要: 入力データ（csv）から簡易ヘッダー付きスペクトルデータ（csv）で出力する
    @param csv_filename : 入力ファイル名
    @param skiprows : ヘッダーでのスキップする行数
    @return raman_spectrum： ramasSPyのオブジェクト
    """

    data = pd.read_csv(csv_filename, skiprows=skiprows)

    x = data["Raman Shift / cm-1"]
    y = data["INT"]

    raman_spectrum = rp.Spectrum(y, x)

    return raman_spectrum





HEADER = "wavenumber, intensity"

def dump_csv(output_csv, data):
    """
    概要: 入力データ（csv）から簡易ヘッダー付きスペクトルデータ（csv）で出力する
    @param output_csv : 出力ファイル名
    @param csv_data : 数値部のデータ
    @return None.
    """
    x = data.spectral_axis
    y = data.spectral_data
    
    spectrum = np.c_[x,y]
    
    np.savetxt(output_csv,
               spectrum,
               delimiter=',', 
               header = HEADER
              )





files = sorted(Path('data').glob('*.csv'))


len(files)








# ラマンスペクトルのクロッピングの範囲指定
CROP = (200, 3400)

# Savitzky-Golayフィルターのウィンドウ幅
WINDOWS_LENGTH = 15

# Savitzky-Golayフィルターの多項式項
POLYORDER = 7

def preprossesing (spectrum):
    
    pipeline = rp.preprocessing.Pipeline([
        rp.preprocessing.baseline.ASPLS(),                                 # ベースライン補正
        rp.preprocessing.despike.WhitakerHayes(),                          # 宇宙線削除
        rp.preprocessing.denoise.SavGol(window_length=WINDOWS_LENGTH, polyorder=POLYORDER),    # Savitzky-Golayフィルター
        rp.preprocessing.normalise.MinMax(),                               # 規格化 standard normal variate（SNV）
        rp.preprocessing.misc.Cropper(region=(CROP))                       # クロッピング
        ])

    correct_spectrum = pipeline.apply(spectrum)

    return correct_spectrum





%%time

data = []
sample_name = []
OUT_DIR =Path("output")

for csvfile in files:

    #スペクトルファイルの読み込み
    spectrum = read_csv(csvfile, skiprows = 1)
    
    # 前処理のパイプライン
    correct = preprossesing(spectrum)

    # 前処理済スペクトルの保存
    output_csv = OUT_DIR.joinpath(csvfile.name[:-4] + "_correct.csv")
    dump_csv(output_csv, correct)

    #データ行列の作成
    data.append(correct.spectral_data)
    sample_name.append(csvfile.name[:-4])





df = pd.DataFrame(data, 
                  index = sample_name, 
                  columns = correct.spectral_axis
                 )
df











from scipy.cluster.hierarchy import linkage, dendrogram
from sklearn.cluster import KMeans





plt.figure(figsize=(8, 16))

# 階層クラスタリング
linkage_matrix = linkage(df, method='ward')

dendrogram(linkage_matrix, 
           labels=df.index, 
           orientation='right',
           color_threshold=20.0,
          )

plt.xlabel('Distance')

plt.show()














from sklearn.decomposition import PCA





# PCAによる次元削減
pca = PCA(n_components=2)
pca.fit(df)
df_t = pca.transform(df)


df_pca = pd.DataFrame(df_t)








fig, ax = plt.subplots(figsize=(6,6))

sns.scatterplot(data=df_pca,
                x=df_pca[0], 
                y=df_pca[1]
               )
ax.set_xlabel('PC1')
ax.set_ylabel('PC2')

plt.grid()
plt.show()





#クラスタごとの誤差平方和の格納する
sse = []

for i  in range(1,11):            
    model = KMeans(n_clusters=i, random_state=42)
    model.fit(df_pca)                         
    sse.append(model.inertia_)  





fig, ax = plt.subplots(figsize=(6,6))

ax.plot(range(1,11),sse,marker='o')

ax.set_xlabel('Number of Clusters')
ax.set_ylabel('SSE')

plt.grid()
plt.show()





# クラスタ数
k = 3

model = KMeans(n_clusters = k, 
               random_state=42
              )

model.fit(df_t)
pred = model.predict(df_t)

df_pca["label"] = pred


fig, ax = plt.subplots(figsize=(6,6))

sns.scatterplot(data=df_pca,
                x=df_pca[0], 
                y=df_pca[1],
                hue="label",
                palette="Set1"
               )

ax.set_xlabel('PC1')
ax.set_ylabel('PC2')

plt.grid()
plt.show()





# 可視化

fig, ax = plt.subplots(figsize=(10,10))

df_pca["label"] = pred

sns.scatterplot(data=df_pca,
                x=df_pca[0], 
                y=df_pca[1],
                hue="label",
                palette="Set1"
               )

ax.set_xlabel('PC1')
ax.set_ylabel('PC2')

data_x = df_pca[0]
data_y = df_pca[1]

labels = df.index


for x, y, l in zip(data_x, data_y, labels):
    plt_text = ax.annotate(l, (x, y), fontsize=8, color='black')

plt.grid()
plt.show()





# 可視化
from adjustText import adjust_text

texts = []
fig, ax = plt.subplots(figsize=(10,10))

sns.scatterplot(data=df_pca,
                x=df_pca[0], 
                y=df_pca[1],
                hue="label",
                palette="Set1"
               )
ax.set_xlabel('PC1')
ax.set_ylabel('PC2')

data_x = df_pca[0]
data_y = df_pca[1]
labels = df.index

for x, y, l in zip(data_x, data_y, labels):
    plt_text = ax.annotate(l, (x, y), fontsize=8, color='black')
    texts.append(plt_text)
 
adjust_text(texts)
plt.grid()
plt.show()





# 可視化
from adjustText import adjust_text

texts = []
fig, ax = plt.subplots(figsize=(10,10))

sns.scatterplot(data=df_pca,
                x=df_pca[0], 
                y=df_pca[1],
                hue="label",
                palette="Set1"
               )

ax.set_xlabel('PC1')
ax.set_ylabel('PC2')

data_x = df_pca[0]
data_y = df_pca[1]
labels = df.index

for x, y, l in zip(data_x, data_y, labels):
    plt_text = ax.annotate(l, (x, y), fontsize=8, color='black')
    texts.append(plt_text)
 
# arrowpropsでラベルからプロットした点を指す線などの形状や色を指定
adjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray', lw=0.5))

plt.grid()
plt.show()



